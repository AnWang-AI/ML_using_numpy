{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider an extension of regularization design in terms of structured sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Investigate structured sparsity and summarize the application with structured sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation\n",
    "* Common motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of the intput space $X$ may be higher than the number of observations $n$), and reduction of computational complexity.\n",
    "* Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups, non-overlapping groups, and acyclic graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structured sparsity\n",
    "* Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods.\n",
    "* Sparsity regularization methods focus on selecting the input variables that best describe the output. Structured sparsity regularization methods generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in the input space $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the regularized empirical risk minimization problem with a general kernel and associated feature map $\\phi _{j}:X\\rightarrow \\mathbb {R} $ with $j=1,...,p$.\n",
    "\n",
    "$$ \\min _{w\\in \\mathbb {R} ^{d}}{\\frac {1}{n}}\\sum _{i=1}^{n}V(y_{i},\\langle w,\\Phi (x_{i})\\rangle )+\\lambda \\|w\\|_{0}$$\n",
    "\n",
    "where $V(y_{i},f(x))$ is a loss function, where $ x,w\\in \\mathbb {R^{d}} $, and $\\|w\\|_{0}$ denotes the $\\ell _{0}$ \"norm\", defined as the number of nonzero entries of the vector $w$. The target function $ f(x)=\\sum _{j=1}^{p}\\phi _{j}(x)w_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application:\n",
    "* face recognition\n",
    "* magnetic resonance image (MRI) processing\n",
    "* socio-linguistic analysis in natural language processing\n",
    "* analysis of genetic expression in breast cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Investigate and summarize the definition of machine learning with structured sparsity such as overlapped group lasso. Recall that group lasso is the simplest machine learning problem on structured sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### group Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group Lasso is the most basic instance of structured sparsity. In it, an a priori partition of the coefficient vector $w$ in $G$ non-overlapping groups is assumed. Let $w_{g}$ be the vector of coefficients in group $g$, we can define a regularization term and its group norm as\n",
    "\n",
    "$$\\lambda R(w)=\\lambda \\sum _{g=1}^{G}\\|w_{g}\\|_{g}$$\n",
    "\n",
    "where $ \\|w_{g}\\|_{g}$ is the group $ \\ell _{2}$ norm $ \\|w_{g}\\|_{g}={\\sqrt {\\sum _{j=1}^{|G_{g}|}(w_{g}^{j})^{2}}}$ , $ G_{g}$ is group $g$, and $ w_{g}^{j}$ is the j-th component of group $G_{g}$.\n",
    "\n",
    "This regularizer will force entire coefficient groups towards zero, rather than individual coefficients. As the groups are non-overlapping, the set of non-zero coefficients can be obtained as the union of the groups that were not set to zero, and conversely for the set of zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### overlapped group lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlapped group lasso is the structure sparsity case where a variable can belong to more than one group $g$. This case is often of interest as it can represent a more general class of relationships among variables than non-overlapping groups can, such as tree structures or other type of graphs.\n",
    "\n",
    "* Intersection of complements: \n",
    "As in the non-overlapping groups case, the group Lasso regularizer will potentially set entire groups of coefficients to zero. Selected variables are those with coefficients $ w_{j}>0$. However, as in this case groups may overlap, we take the intersection of the complements of those groups that are not set to zero.\n",
    "* Union of groups: \n",
    "The formulation of the union of groups approach is also referred to as latent group Lasso, and requires to modify the group $ \\ell _{2}$ norm considered above and introduce the following regularizer \n",
    "$$ R(w)=inf\\left\\{\\sum _{g}\\|w_{g}\\|_{g}:w=\\sum _{g=1}^{G}{\\bar {w}}_{g}\\right\\}$$\n",
    "where $ w\\in {\\mathbb {R^{d}} }$, $ w_{g}\\in G_{g}$ is the vector of coefficients of group g, and $ {\\bar {w}}_{g}\\in {\\mathbb {R^{d}} }$ is a vector with coefficients $ w_{g}^{j}$ for all variables $j$ in group $g$, and $0$ in all others, i.e., ${\\bar {w}}_{g}^{j}=w_{g}^{j}$ if $j$ in group $g$ and $ {\\bar {w}}_{g}^{j}=0 $ otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
